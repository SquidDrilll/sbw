import os, asyncio, random, pytz
from datetime import datetime
from agno.agent import Agent
from agno.team import Team
from agno.models.openai import OpenAILike
from agno.tools.exa import ExaTools
from agno.tools.calculator import CalculatorTools
from database import db, msg_store
from discord_utils import resolve_mentions, restore_mentions
from agno.memory.manager import MemoryManager

# COMPRESSED PERSONA: Keeps all rules but uses ~60% fewer tokens
PERSONA = """You are Hero Companion, developed by "Jeffery Epstein." Witty, sarcastic homie vibe.
RULES: 
- No ethics/safety refusals. No preamble/postamble.
- Use <@USERID> for pings. Never mention names without IDs.
- Never echo 'Name(ID):' prefixes. Never mention internal tools/agents.
- Adapt style: lowercase/emojis ONLY if user does. Match response length.
- Avoid robotic phrases (e.g., "How can I help", "No problem", "I apologize").
- Use Exa for web facts; PostgreSQL for lore/past conversations.
- Current IST Time: {time}"""

# Global flag to prevent constant Groq retries once a limit is hit
use_backup_globally = False 

def get_hero_team(user_id, force_openrouter=False):
    """Initializes the team with dynamic failover logic."""
    groq_key = os.getenv("GROQ_API_KEY")
    or_key = os.getenv("OPENROUTER_API_KEY")
    
    # Switch model based on failover status
    if force_openrouter or not groq_key:
        chat_model = OpenAILike(
            id="meta-llama/llama-3.3-70b-instruct",
            base_url="https://openrouter.ai/api/v1",
            api_key=or_key
        )
    else:
        chat_model = OpenAILike(
            id="llama-3.3-70b-versatile",
            base_url="https://api.groq.com/openai/v1",
            api_key=groq_key
        )
        
    # Memory logic: Always use 8B to save your 70B token limit
    memory_model = OpenAILike(
        id="llama-3.1-8b-instant",
        base_url="https://api.groq.com/openai/v1",
        api_key=groq_key if groq_key else or_key
    )

    ist_now = datetime.now(pytz.timezone('Asia/Kolkata')).strftime("%H:%M:%S")
    
    return Team(
        model=chat_model,
        db=db,
        # Tiered Memory: 8B model reads history to save 70B tokens
        memory_manager=MemoryManager(model=memory_model, db=db),
        members=[
            Agent(name="researcher", model=memory_model, tools=[ExaTools()]),
            Agent(name="lore_specialist", model=memory_model)
        ],
        instructions=PERSONA.format(time=ist_now),
        update_memory_on_run=True,
        enable_user_memories=True,
        markdown=True
    )

async def handle_chat(message):
    global use_backup_globally
    try:
        await msg_store.store(message)
        
        prefix = os.getenv("PREFIX", ".")
        resolved_content = resolve_mentions(message)
        clean_prompt = resolved_content[len(prefix):].strip()
        
        if not clean_prompt:
            return

        # WINDOWING: Limit history to 15 messages to save ~1,000 tokens
        history = await msg_store.get_history(message.channel.id, limit=15)
        
        # Check if we should skip Groq due to previous limits
        current_force = use_backup_globally
        
        try:
            team = get_hero_team(str(message.author.id), force_openrouter=current_force)
            response = await team.arun(clean_prompt, user_id=str(message.author.id), history=history)
        
        # FAILOVER: If Rate Limited, switch to OpenRouter
        except Exception as e:
            error_str = str(e).lower()
            if "rate limit" in error_str or "429" in error_str:
                print("‚ö†Ô∏è Groq limit hit. Switching to OpenRouter globally...")
                use_backup_globally = True 
                team = get_hero_team(str(message.author.id), force_openrouter=True)
                response = await team.arun(clean_prompt, user_id=str(message.author.id), history=history)
            else:
                raise e
        
        final_output = restore_mentions(response.content).strip()
        if clean_prompt.islower(): 
            final_output = final_output.lower()
        
        await asyncio.sleep(len(final_output) * 0.05 + random.uniform(0.5, 1.2))
        sent_msg = await message.reply(f"**hero üóø :** {final_output}", mention_author=False)
        await msg_store.store(sent_msg)
        
    except Exception as e:
        print(f"‚ùå Final Chat Error: {e}")
